# -*- coding: utf-8 -*-
"""Signals.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZojF6RX7-giYAMexKoKdEQuJT5NaguQr
"""

pip install requests beautifulsoup4 newspaper3k googlesearch-python transformers

import requests
from bs4 import BeautifulSoup
from newspaper import Article
from googlesearch import search
from transformers import pipeline
import nltk

# Download the NLTK punkt tokenizer
nltk.download('punkt')

# Initialize the Hugging Face transformer model
summarizer = pipeline("summarization")
classifier = pipeline("zero-shot-classification")

# Define the labels for classification
labels = ["potential sale", "not potential sale"]

def scrape_bloomberg():
    url = "https://www.bloomberg.com/"
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        articles = soup.find_all('article')
        return extract_articles(articles)
    else:
        print("Failed to retrieve data from Bloomberg.")
        return []

def scrape_reuters():
    url = "https://www.reuters.com/"
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        articles = soup.find_all('article')
        return extract_articles(articles)
    else:
        print("Failed to retrieve data from Reuters.")
        return []

def scrape_cnbc():
    url = "https://www.cnbc.com/"
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        articles = soup.find_all('article')
        return extract_articles(articles)
    else:
        print("Failed to retrieve data from CNBC.")
        return []

def scrape_yahoo_finance():
    url = "https://finance.yahoo.com/"
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        articles = soup.find_all('article')
        return extract_articles(articles)
    else:
        print("Failed to retrieve data from Yahoo Finance.")
        return []

def scrape_insurance_journal():
    url = "https://www.insurancejournal.com/"
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        articles = soup.find_all('article')
        return extract_articles(articles)
    else:
        print("Failed to retrieve data from Insurance Journal.")
        return []

def scrape_broad_web_search(query):
    search_results = search(query, num_results=10)
    relevant_articles = []
    for url in search_results:
        news_article = Article(url)
        try:
            news_article.download()
            news_article.parse()
            news_article.nlp()
            article_text = news_article.text

            classification = classifier(article_text, labels)
            if classification['labels'][0] == "potential sale" and classification['scores'][0] > 0.5:
                summary = summarizer(article_text, max_length=100, min_length=30, do_sample=False)
                relevant_articles.append({
                    "title": news_article.title,
                    "url": url,
                    "summary": summary[0]['summary_text']
                })
        except:
            print(f"Failed to download or parse article: {url}")
    return relevant_articles

def extract_articles(articles):
    relevant_articles = []
    for article in articles:
        title_element = article.find('h1') or article.find('h2') or article.find('h3')
        if title_element:
            title = title_element.text.strip()
            url_element = article.find('a', href=True)
            if url_element:
                article_url = url_element['href']

                news_article = Article(article_url)
                try:
                    news_article.download()
                    news_article.parse()
                    news_article.nlp()
                    article_text = news_article.text

                    classification = classifier(article_text, labels)
                    if classification['labels'][0] == "potential sale" and classification['scores'][0] > 0.5:
                        summary = summarizer(article_text, max_length=100, min_length=30, do_sample=False)
                        relevant_articles.append({
                            "title": title,
                            "url": article_url,
                            "summary": summary[0]['summary_text']
                        })
                except:
                    print(f"Failed to download or parse article: {article_url}")
    return relevant_articles

def scrape_all_sources():
    all_relevant_articles = []
    all_relevant_articles.extend(scrape_bloomberg())
    all_relevant_articles.extend(scrape_reuters())
    all_relevant_articles.extend(scrape_cnbc())
    all_relevant_articles.extend(scrape_yahoo_finance())
    all_relevant_articles.extend(scrape_insurance_journal())
    all_relevant_articles.extend(scrape_broad_web_search("insurance company acquisition"))
    return all_relevant_articles

# Main function
if __name__ == "__main__":
    relevant_articles = scrape_all_sources()

    if relevant_articles:
        print("Relevant Articles:")
        for article in relevant_articles:
            print("Title:", article["title"])
            print("URL:", article["url"])
            print("Summary:", article["summary"])
            print()
    else:
        print("No relevant articles found.")